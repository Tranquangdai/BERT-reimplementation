{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch, os\n",
    "import numpy as np, six, re\n",
    "from modeling import *\n",
    "from easydict import EasyDict as edict\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-04 18:49:14--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.111.128\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.111.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 407727028 (389M) [application/zip]\n",
      "Saving to: ‘models/uncased_L-12_H-768_A-12.zip’\n",
      "\n",
      "uncased_L-12_H-768_ 100%[===================>] 388.84M  6.30MB/s    in 71s     \n",
      "\n",
      "2020-05-04 18:50:26 (5.47 MB/s) - ‘models/uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
      "\n",
      "Archive:  models/uncased_L-12_H-768_A-12.zip\n",
      "   creating: models/uncased_L-12_H-768_A-12/\n",
      "  inflating: models/uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
      "  inflating: models/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
      "  inflating: models/uncased_L-12_H-768_A-12/vocab.txt  \n",
      "  inflating: models/uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
      "  inflating: models/uncased_L-12_H-768_A-12/bert_config.json  \n"
     ]
    }
   ],
   "source": [
    "!wget -P models/ https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "!unzip -d models/ models/uncased_L-12_H-768_A-12.zip \n",
    "!rm -rf models/uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/uncased_L-12_H-768_A-12/bert_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf_path = os.path.abspath('models/uncased_L-12_H-768_A-12/bert_model.ckpt')\n",
    "config = BertConfig.from_json_file('models/uncased_L-12_H-768_A-12/bert_config.json')\n",
    "with open('models/uncased_L-12_H-768_A-12/bert_config.json', 'r') as f:\n",
    "    _config = edict(json.load(f))\n",
    "\n",
    "tf_vars = tf.train.list_variables(tf_path)\n",
    "def list_tf_var_by_scope(scope):\n",
    "    return [i for i in tf_vars if scope in i[0]]\n",
    "\n",
    "def load_tf_var_as_numpy(name):\n",
    "    return tf.train.load_variable(tf_path, name)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.import_meta_graph('models/uncased_L-12_H-768_A-12/bert_model.ckpt.meta')\n",
    "saver.restore(sess, 'models/uncased_L-12_H-768_A-12/bert_model.ckpt')\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "node_name = [i.name for i in graph.as_graph_def().node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_matrix(input_tensor):\n",
    "    ndims = len(input_tensor.shape)\n",
    "    if ndims < 2:\n",
    "        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %(input_tensor.shape))\n",
    "    if ndims == 2:\n",
    "        return input_tensor\n",
    "    \n",
    "    width = input_tensor.shape[-1]\n",
    "    output_tensor = torch.reshape(input_tensor, [-1, width])\n",
    "    return output_tensor\n",
    "\n",
    "def reshape_from_matrix(output_tensor, orig_shape_list):\n",
    "    if len(orig_shape_list) == 2:\n",
    "        return output_tensor\n",
    "    output_shape = list(output_tensor.shape)\n",
    "    \n",
    "    orig_dims = orig_shape_list[0:-1]\n",
    "    width = output_shape[-1]\n",
    "    return torch.reshape(output_tensor, orig_dims + [width])\n",
    "\n",
    "def assert_rank(tensor, expected_rank, name=None):\n",
    "    \n",
    "    if name is None:\n",
    "        name = tensor.name\n",
    "        \n",
    "    expected_rank_dict = {}\n",
    "    if isinstance(expected_rank, six.integer_types):\n",
    "        expected_rank_dict[expected_rank] = True\n",
    "    else:\n",
    "        for x in expected_rank:\n",
    "            expected_rank_dict[x] = True\n",
    "    actual_rank = len(tensor.shape)\n",
    "    if actual_rank not in expected_rank_dict:\n",
    "        raise ValueError(\"For the tensor `%s`, the actual rank `%d` (shape = %s) is not equal to the expected rank `%s`\" %\n",
    "                        (name, actual_rank, str(tensor.shape), str(expected_rank)))\n",
    "        \n",
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "    if name is None:\n",
    "        name = tensor.name\n",
    "        \n",
    "    if expected_rank is not None:\n",
    "        assert_rank(tensor, expected_rank, name)\n",
    "        \n",
    "    shape = list(tensor.shape)\n",
    "    non_static_indexes = []\n",
    "    for (index, dim) in enumerate(shape):\n",
    "        if dim is None:\n",
    "            non_static_indexes.append(index)\n",
    "\n",
    "    if not non_static_indexes:\n",
    "        return shape\n",
    "\n",
    "    dyn_shape = torch.shape(tensor)\n",
    "    for index in non_static_indexes:\n",
    "        shape[index] = dyn_shape[index]\n",
    "    return shape\n",
    "    \n",
    "def gelu(x):\n",
    "    cdf = 0.5*(1.0 + torch.tanh(\n",
    "        np.sqrt(2/np.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    return x * cdf\n",
    "\n",
    "def create_attention_mask_from_input_mask(from_tensor, to_mask):\n",
    "    from_shape = get_shape_list(from_tensor, expected_rank=[2,3])\n",
    "    batch_size = from_shape[0]\n",
    "    from_seq_length = from_shape[1]\n",
    "\n",
    "    to_shape = get_shape_list(to_mask, expected_rank=2)\n",
    "    to_seq_length = to_shape[1]\n",
    "    \n",
    "    to_mask = torch.reshape(to_mask, [batch_size, 1, to_seq_length]).type(torch.FloatTensor)    \n",
    "    broadcast_ones = torch.ones([batch_size, from_seq_length, 1]).type(torch.FloatTensor)\n",
    "    mask = broadcast_ones * to_mask\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSelf(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config,\n",
    "                 do_return_2d_tensor=True, \n",
    "                 batch_size=1,\n",
    "                 num_attention_heads=12, \n",
    "                 size_per_head=64,\n",
    "                 from_seq_length=128,\n",
    "                 to_seq_length=128):\n",
    "        \n",
    "        super(AttentionSelf, self).__init__()\n",
    "        self.query = nn.Linear(config.hidden_size, config.hidden_size, bias=True)\n",
    "        self.key = nn.Linear(config.hidden_size, config.hidden_size, bias=True)\n",
    "        self.value = nn.Linear(config.hidden_size, config.hidden_size, bias=True)\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        \n",
    "        self.do_return_2d_tensor = do_return_2d_tensor\n",
    "        self.batch_size = batch_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.size_per_head = size_per_head\n",
    "        self.from_seq_length = from_seq_length\n",
    "        self.to_seq_length = to_seq_length\n",
    "        \n",
    "    def transpose_for_scores(self, \n",
    "                            input_tensor, batch_size=1, seq_length=128, \n",
    "                             num_attention_heads=12, width=64):\n",
    "        output_tensor = torch.reshape(input_tensor, \n",
    "                    [batch_size, seq_length, num_attention_heads, width])\n",
    "        output_tensor = output_tensor.permute(0, 2, 1, 3)\n",
    "        return output_tensor\n",
    "        \n",
    "    def forward(self, from_tensor, to_tensor, attention_mask):\n",
    "        \n",
    "        query_layer = self.query(from_tensor)\n",
    "        key_layer = self.key(to_tensor)\n",
    "        value_layer = self.value(to_tensor)\n",
    "        \n",
    "        query_layer = self.transpose_for_scores(query_layer, \n",
    "            self.batch_size, self.from_seq_length, self.num_attention_heads, self.size_per_head)\n",
    "        key_layer = self.transpose_for_scores(key_layer, \n",
    "            self.batch_size, self.to_seq_length, self.num_attention_heads, self.size_per_head)\n",
    "        \n",
    "        attention_scores = torch.matmul(query_layer, torch.transpose(key_layer, -1, -2))\n",
    "        attention_scores /= math.sqrt(self.size_per_head)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_mask = torch.unsqueeze(attention_mask, dim=1)\n",
    "            adder = (1.0 - attention_mask) * -10000.0\n",
    "            attention_scores += adder\n",
    "            \n",
    "        attention_probs = F.softmax(attention_scores, -1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        \n",
    "        value_layer = torch.reshape(value_layer, \n",
    "            [self.batch_size, self.to_seq_length, self.num_attention_heads, self.size_per_head])\n",
    "        value_layer = value_layer.permute(0, 2, 1, 3)\n",
    "        \n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3)\n",
    "        if self.do_return_2d_tensor:\n",
    "            context_layer = torch.reshape(context_layer, \n",
    "            [self.batch_size * self.from_seq_length, self.num_attention_heads * self.size_per_head])\n",
    "        else:\n",
    "            context_layer = torch.reshape(context_layer, \n",
    "            [self.batch_size, self.from_seq_length, self.num_attention_heads * self.size_per_head])\n",
    "        \n",
    "        return context_layer\n",
    "    \n",
    "    \n",
    "class AttentionOutput(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(AttentionOutput, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size, bias=True)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, attention_output, layer_input):\n",
    "        attention_output = self.dense(attention_output)\n",
    "        attention_output = self.dropout(attention_output)\n",
    "        attention_output = self.LayerNorm(attention_output+layer_input)\n",
    "        return attention_output\n",
    "    \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(Attention, self).__init__()\n",
    "        self.self = AttentionSelf(config)\n",
    "        self.output = AttentionOutput(config)\n",
    "        \n",
    "    def forward(self, from_tensor, to_tensor, attention_mask):\n",
    "        attention_heads = []\n",
    "        attention_head = self.self(\n",
    "                                        from_tensor=from_tensor, \n",
    "                                        to_tensor=to_tensor, \n",
    "                                        attention_mask=attention_mask)\n",
    "        attention_heads.append(attention_head)\n",
    "        attention_output = None\n",
    "        if len(attention_heads) == 1:\n",
    "            attention_output = attention_heads[0]\n",
    "        else:\n",
    "            attention_output = torch.cat(attention_heads, dim=-1)\n",
    "            \n",
    "        attention_output = self.output(attention_output, from_tensor)\n",
    "        return attention_output\n",
    "    \n",
    "    \n",
    "class Intermediate(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(Intermediate, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.intermediate_act_fn = gelu\n",
    "        \n",
    "    def forward(self, activation_output):\n",
    "        return self.intermediate_act_fn(self.dense(activation_output))\n",
    "    \n",
    "    \n",
    "class Output(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(Output, self).__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size)\n",
    "        \n",
    "    def forward(self, intermediate_output, attention_output):\n",
    "        layer_output = self.dense(intermediate_output)\n",
    "        layer_output = self.dropout(layer_output)\n",
    "        layer_output = self.LayerNorm(layer_output + attention_output)\n",
    "        return layer_output\n",
    "    \n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = Attention(config)\n",
    "        self.intermediate = Intermediate(config)\n",
    "        self.output = Output(config)\n",
    "        \n",
    "    def forward(self, layer_input, attention_mask):\n",
    "        attention_output = self.attention(from_tensor=layer_input, \n",
    "                                          to_tensor=layer_input, \n",
    "                                          attention_mask=attention_mask)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size)\n",
    "    \n",
    "    def forward(self, input_ids=None, token_type_ids=None, position_ids=None):\n",
    "        word_embeddings = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        return self.LayerNorm(word_embeddings+token_type_embeddings+position_embeddings)\n",
    "    \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config, do_return_all_layers=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layer = nn.ModuleList([TransformerBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.do_return_all_layers = do_return_all_layers\n",
    "        \n",
    "    def forward(self, input_tensor, attention_mask):\n",
    "        \n",
    "        input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
    "        prev_output = reshape_to_matrix(input_tensor)\n",
    "        \n",
    "        all_layer_outputs = []\n",
    "        for layer in self.layer:\n",
    "            layer_input = prev_output\n",
    "            layer_output = layer(layer_input, attention_mask)\n",
    "            prev_output = layer_output\n",
    "            all_layer_outputs.append(layer_output)\n",
    "            \n",
    "        if self.do_return_all_layers:\n",
    "            final_outputs = []\n",
    "            for layer_output in all_layer_outputs:\n",
    "                final_output = reshape_from_matrix(layer_output, input_shape)\n",
    "                final_outputs.append(final_output)\n",
    "            return final_outputs\n",
    "        else:\n",
    "            final_output = reshape_from_matrix(prev_output, input_shape)\n",
    "            return final_output\n",
    "        \n",
    "        \n",
    "class Pooler(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        \n",
    "    def forward(self, sequence_output):\n",
    "        first_token_tensor = torch.squeeze(sequence_output[:, 0:1, :], dim=0)\n",
    "        pooled_output = torch.tanh(self.dense(first_token_tensor))\n",
    "        return pooled_output\n",
    "        \n",
    "        \n",
    "class BertModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.encoder = Encoder(config)\n",
    "        self.pooler = Pooler(config)\n",
    "        \n",
    "    def forward(self, input_ids, input_mask=None, token_type_ids=None):\n",
    "        \n",
    "        input_shape = get_shape_list(input_ids, expected_rank=2)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_length = input_shape[1]\n",
    "        \n",
    "        if input_mask is None:\n",
    "            input_mask = torch.ones([batch_size, seq_length]).type(torch.LongTensor)\n",
    "        \n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros([batch_size, seq_length]).type(torch.LongTensor)\n",
    "        \n",
    "        # embeddings scope\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids, position_ids)\n",
    "        \n",
    "        # encoder scope\n",
    "        attention_mask = create_attention_mask_from_input_mask(input_mask, input_ids) \n",
    "        \n",
    "        all_encoder_layers = self.encoder(embedding_output, attention_mask)\n",
    "        sequence_output = all_encoder_layers[-1]\n",
    "        \n",
    "        # pooler scope\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_tf2torch(text=None, model_name='bert'):\n",
    "    if 'LayerNorm' in text:\n",
    "        if 'weight' in text:\n",
    "            text = text.replace('weight', 'gamma')\n",
    "        elif 'bias' in text:\n",
    "            text = text.replace('bias', 'beta')\n",
    "    else:\n",
    "        text = text.replace('weight', 'kernel')\n",
    "    layer_idx = re.findall('layer.\\d+', text)\n",
    "    if len(layer_idx):\n",
    "        layer_idx = layer_idx[0]\n",
    "        new_layer_idx = layer_idx.replace('.', '_')\n",
    "        text = text.replace(layer_idx, new_layer_idx)\n",
    "    if '_embeddings' in text:\n",
    "        text = text.replace('kernel', '')\n",
    "    return '/'.join([model_name, text.replace('.', '/')]).strip('/')\n",
    "\n",
    "def load_bert_weights(tf_path, torch_model, model_name):\n",
    "    \n",
    "    def _import_weights(tf_path, name):\n",
    "        arr = tf.train.load_variable(tf_path, name)\n",
    "        if 'kernel' in name:\n",
    "            return arr.transpose()\n",
    "        return arr\n",
    "    \n",
    "    loaded_state_dict = OrderedDict()\n",
    "    for i in list(torch_model.state_dict().keys()):\n",
    "        torch_weight = _import_weights(tf_path, layer_tf2torch(i, model_name))\n",
    "        loaded_state_dict[i] = torch.tensor(torch_weight)\n",
    "    return loaded_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertmodel = BertModel(config)\n",
    "bertmodel.eval()\n",
    "loaded_state_dict = load_bert_weights(tf_path, bertmodel, 'bert')\n",
    "bertmodel.load_state_dict(loaded_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9556e-01, -3.8529e-01, -1.2395e-01,  6.4992e-01,  3.9835e-01,\n",
      "         -2.6433e-01,  4.4037e-01,  1.1338e-01, -5.4311e-01, -4.8537e-01,\n",
      "         -4.6098e-01,  9.5167e-01,  9.2152e-01, -4.9869e-01,  9.0811e-01,\n",
      "          3.6789e-02, -6.3752e-01,  2.1472e-01,  3.2824e-01,  5.0361e-01,\n",
      "          8.3490e-01,  9.9977e-01, -5.0796e-02,  1.4625e-01,  1.5398e-01,\n",
      "          9.9142e-01, -7.9022e-01,  8.9986e-01,  8.4503e-01,  3.2737e-01,\n",
      "         -9.3161e-02,  1.0036e-01, -9.8784e-01, -2.3155e-01, -8.6401e-01,\n",
      "         -9.0584e-01,  1.9727e-01, -2.7658e-01,  2.8379e-03,  8.9015e-02,\n",
      "         -8.9106e-01,  2.2820e-01,  8.8577e-01,  3.9246e-01,  8.1413e-01,\n",
      "          9.0167e-02, -9.9637e-01,  2.1161e-01, -6.1490e-01,  5.2413e-01,\n",
      "          5.7233e-01,  9.6379e-01,  2.2309e-02,  2.5973e-01,  2.5539e-01,\n",
      "         -3.1383e-01,  7.0062e-02,  9.4177e-02,  2.1852e-02, -3.6695e-01,\n",
      "         -4.2327e-01,  4.3701e-01, -6.9330e-01, -2.9266e-01,  8.6426e-01,\n",
      "         -4.9095e-01, -2.5013e-01, -1.6440e-01, -9.6667e-02, -1.4855e-01,\n",
      "          4.3344e-01,  3.9451e-01,  6.0425e-01, -8.8457e-01, -3.5195e-01,\n",
      "          2.1016e-01,  1.1817e-01,  9.9999e-01, -3.7809e-01, -9.6995e-01,\n",
      "          9.1413e-01, -2.1677e-01, -1.2232e-01,  6.2767e-01, -5.1395e-01,\n",
      "         -9.9982e-01,  5.0144e-01, -8.3271e-03, -9.4092e-01,  1.5171e-01,\n",
      "          6.2897e-01, -1.1104e-01,  4.6771e-01, -1.0726e-01,  4.6175e-01,\n",
      "         -7.7311e-01,  5.2961e-03, -8.6711e-01, -3.5089e-01, -3.6073e-01,\n",
      "          2.7999e-01, -1.1450e-02, -3.3996e-01,  1.3723e-01,  2.2728e-01,\n",
      "         -7.9420e-02, -6.4965e-01,  2.3282e-01, -7.4586e-01,  1.9084e-01,\n",
      "          1.6478e-03, -3.3824e-01, -1.4709e-01, -8.4623e-01,  1.5478e-01,\n",
      "          9.0395e-02, -9.5806e-01,  1.5368e-01, -9.6498e-01,  6.3557e-01,\n",
      "         -6.4592e-01, -2.7767e-01,  8.5201e-01, -8.7225e-01,  1.6965e-01,\n",
      "         -7.3225e-02, -8.9889e-01, -9.9999e-01, -1.5756e-01, -4.2787e-01,\n",
      "         -6.9096e-02, -3.3124e-01, -8.9210e-01, -9.4385e-01,  2.3197e-01,\n",
      "          7.2375e-01,  2.0528e-01,  8.8045e-01, -6.7547e-01,  8.8406e-01,\n",
      "          2.7083e-01, -6.1766e-01,  5.8812e-01, -2.1348e-01,  9.4315e-01,\n",
      "          3.1071e-01,  2.3834e-01,  2.4695e-02, -5.1947e-01,  2.7976e-01,\n",
      "         -8.1474e-01,  1.6210e-01,  3.4392e-01, -6.9059e-01,  1.3796e-01,\n",
      "          8.8331e-01, -3.8141e-01, -1.0400e-01,  5.0697e-01,  2.6149e-02,\n",
      "         -5.0339e-01,  2.3896e-01,  8.3879e-01, -1.2339e-01,  1.4870e-01,\n",
      "          3.3364e-01, -7.7988e-01,  5.2971e-01, -4.9280e-01, -6.0441e-01,\n",
      "          1.8459e-01,  3.7407e-02, -3.3220e-01, -9.7550e-01, -1.7479e-01,\n",
      "          2.5524e-01,  9.3989e-01,  2.2045e-01,  7.2416e-02,  6.4296e-01,\n",
      "          1.6315e-01,  4.8751e-01, -9.4451e-01,  9.2268e-01, -4.4861e-01,\n",
      "          2.8079e-01, -8.3319e-01,  5.1071e-01, -3.8514e-01, -4.3636e-01,\n",
      "          5.7696e-01, -7.0711e-01, -5.2330e-01, -2.1641e-01, -1.6964e-01,\n",
      "         -1.7508e-01, -9.2582e-01, -3.2914e-01, -2.5793e-01, -2.6965e-02,\n",
      "         -1.7159e-01,  8.5537e-01, -3.4944e-01,  3.8656e-01,  6.2773e-01,\n",
      "          4.1225e-01, -4.2121e-01, -5.2432e-01, -1.0415e-01, -1.5291e-01,\n",
      "          3.2288e-01,  9.6510e-01, -7.7658e-01,  1.9118e-01, -4.9161e-01,\n",
      "         -9.2773e-01,  4.1431e-02, -2.8202e-01, -2.2514e-01, -5.7090e-01,\n",
      "          3.7794e-01, -9.2380e-01, -6.9475e-01,  3.6926e-01, -1.1605e-01,\n",
      "         -7.4643e-01,  6.4262e-02, -2.9828e-01,  4.2299e-01, -1.3623e-04,\n",
      "          9.9365e-01,  7.4675e-01, -3.0227e-01, -2.2331e-01,  9.3503e-01,\n",
      "         -9.5203e-01, -7.7138e-01,  5.0315e-01,  1.1170e-01, -9.5738e-02,\n",
      "         -3.0958e-01,  3.1593e-02,  9.2671e-01,  8.7812e-01, -7.8147e-01,\n",
      "         -5.2100e-01,  9.2919e-01,  6.0013e-01, -2.5670e-01, -6.5689e-01,\n",
      "         -3.0412e-01, -1.5795e-01,  8.2654e-02,  8.9268e-01, -5.6766e-01,\n",
      "          9.2777e-02, -9.9335e-01, -8.5791e-01, -9.8068e-01,  1.5546e-01,\n",
      "         -9.3871e-01,  7.8910e-01,  4.4192e-01,  9.8793e-01, -3.5868e-01,\n",
      "         -3.3736e-01, -9.6048e-01,  3.2610e-01,  2.0033e-01,  4.6963e-02,\n",
      "         -3.7334e-01, -5.0020e-01,  2.9289e-02, -9.6546e-01,  5.3903e-03,\n",
      "         -6.0171e-02,  1.9838e-01,  2.6614e-01, -8.5392e-01,  2.9479e-01,\n",
      "          2.9512e-01,  4.4504e-01,  4.1514e-01,  7.8897e-01,  9.9998e-01,\n",
      "          9.2255e-01,  6.1990e-01, -4.3829e-01, -9.9956e-01, -9.1776e-01,\n",
      "          5.8531e-01, -9.9430e-01, -9.9997e-01, -5.4858e-01, -2.1977e-01,\n",
      "          3.6026e-01, -9.9999e-01, -1.0884e-01,  6.5286e-02, -6.2247e-01,\n",
      "          2.5086e-01,  8.7578e-01,  4.0559e-01, -9.9999e-01, -5.2049e-01,\n",
      "          6.7352e-01,  3.3481e-01,  9.7229e-01, -5.2972e-01,  8.8983e-01,\n",
      "         -6.2604e-02,  6.1743e-01, -2.7118e-01,  4.2217e-01, -6.7833e-01,\n",
      "         -4.4595e-01, -4.2432e-01, -9.2500e-01,  9.9680e-01, -7.0458e-02,\n",
      "         -3.9698e-01, -6.5407e-01,  8.7079e-01, -3.4381e-01,  7.2141e-02,\n",
      "         -9.5904e-01, -3.9598e-02,  6.2121e-01,  8.0256e-01,  3.7941e-01,\n",
      "          1.1514e-01, -4.7389e-02, -1.9997e-01,  1.8512e-01, -1.1360e-01,\n",
      "         -2.0675e-01, -3.6602e-01,  4.8702e-01, -1.5286e-01,  2.3000e-01,\n",
      "          1.5237e-01, -9.6695e-01,  7.7494e-01, -3.5550e-01,  6.1186e-01,\n",
      "          1.0000e+00,  9.3172e-01, -6.1923e-01,  7.9055e-01, -1.3783e-01,\n",
      "          8.7798e-02,  9.9995e-01,  7.0951e-01, -9.4938e-01,  2.5961e-01,\n",
      "         -2.5251e-02, -3.4328e-01, -5.4581e-01,  6.1862e-01, -4.1224e-02,\n",
      "         -4.4112e-01, -6.8122e-01,  9.6926e-01, -9.2908e-01,  9.8109e-01,\n",
      "         -1.2664e-01, -9.4419e-01,  8.2111e-01,  9.1465e-01, -6.8763e-01,\n",
      "          3.0503e-01,  3.1611e-01,  5.6354e-01,  2.1929e-01,  5.1975e-01,\n",
      "          7.5056e-01,  4.5949e-01, -1.0970e-01,  7.8620e-01,  6.2572e-01,\n",
      "          2.5885e-01,  1.1735e-01, -6.3066e-01, -1.3194e-01,  9.0632e-01,\n",
      "          4.0631e-01, -7.8788e-02,  3.1255e-01, -2.9915e-01, -9.6486e-01,\n",
      "         -7.3607e-01,  9.0169e-01,  9.9999e-01, -1.9000e-01,  9.8965e-01,\n",
      "          8.2097e-01, -1.8158e-01,  2.0031e-01,  1.1657e-01,  3.4936e-01,\n",
      "         -1.9414e-01, -2.5486e-01,  7.8655e-01, -5.9302e-01, -9.6598e-01,\n",
      "          1.2225e-01,  1.4217e-01, -3.4714e-01,  9.9299e-01,  3.1421e-01,\n",
      "         -1.1544e-01,  2.9146e-01,  9.8766e-01,  1.4790e-01,  7.9488e-03,\n",
      "         -8.7621e-02,  9.0017e-01,  1.2452e-01, -1.6940e-01, -2.9728e-01,\n",
      "          5.3241e-02, -2.7863e-01, -2.2959e-01,  1.4055e-01, -8.8448e-01,\n",
      "         -7.1457e-02, -8.6788e-01,  8.8223e-01,  9.1692e-01,  5.0542e-01,\n",
      "          3.3206e-01,  9.3608e-01,  9.9999e-01, -9.9242e-01,  1.8736e-01,\n",
      "          9.2452e-01,  4.8778e-01, -9.9777e-01, -5.2268e-01, -1.3003e-01,\n",
      "          4.7425e-02,  9.4261e-02,  4.9279e-02,  6.1676e-02, -6.8651e-01,\n",
      "         -1.1138e-01,  8.4152e-01, -6.5853e-01, -9.3518e-01, -4.0589e-01,\n",
      "          7.1947e-01,  1.8768e-02, -9.9090e-01, -7.9734e-01, -1.7220e-01,\n",
      "          7.6541e-02, -1.0321e-01, -8.7257e-01,  3.0471e-02, -3.1477e-01,\n",
      "          8.5548e-02, -1.2265e-01, -3.4200e-02, -8.7739e-02,  9.0872e-01,\n",
      "         -9.3490e-01, -6.7436e-01, -4.0673e-01, -3.5009e-01,  1.2437e-01,\n",
      "          2.8438e-01, -8.9235e-01, -2.9335e-01,  9.9998e-01, -7.7332e-01,\n",
      "         -1.3217e-01,  7.1286e-01, -5.4231e-02, -3.0546e-01,  2.1560e-01,\n",
      "          8.8052e-01,  4.5370e-01,  6.4562e-01,  1.1743e-01,  7.7179e-01,\n",
      "         -3.5914e-01,  4.1108e-02,  9.4333e-01, -3.6484e-01,  6.6247e-01,\n",
      "          8.4386e-01,  3.6684e-01, -2.5382e-01,  4.1524e-01,  4.0159e-01,\n",
      "         -3.7781e-01, -4.9074e-01,  8.3781e-02, -2.9437e-01, -3.5966e-01,\n",
      "          5.5455e-01,  9.9999e-01,  1.8345e-01,  9.0370e-01, -9.5814e-01,\n",
      "         -8.1660e-01, -1.5401e-03,  9.9998e-01,  8.0516e-01,  6.5427e-01,\n",
      "          6.5317e-01,  8.1969e-01,  1.8729e-02, -2.8045e-01, -2.6376e-02,\n",
      "         -1.5827e-01,  2.7061e-01,  1.4281e-01,  8.9916e-01, -6.0777e-01,\n",
      "         -9.6432e-01, -6.0434e-01,  2.4430e-01, -9.0029e-01,  9.9970e-01,\n",
      "         -2.9348e-01, -1.1169e-01, -2.9160e-01, -5.0093e-01, -9.5682e-01,\n",
      "          6.3082e-02, -9.2790e-01, -4.7848e-01,  2.7002e-01,  9.1231e-01,\n",
      "          3.5671e-02,  1.4921e-01, -7.9262e-01,  8.5465e-01,  3.6111e-01,\n",
      "         -7.5099e-01, -8.7993e-01,  9.2988e-01, -8.7159e-01,  3.7098e-01,\n",
      "          9.9968e-01,  1.1813e-01,  7.8971e-02,  2.0922e-02,  1.4222e-01,\n",
      "          2.4337e-01, -9.4406e-01,  4.0638e-01, -8.8519e-01, -1.0695e-01,\n",
      "         -3.2479e-01,  3.4110e-01, -2.9763e-02, -9.8894e-01,  5.4062e-01,\n",
      "          2.5879e-01,  3.4173e-01, -2.7300e-01, -1.8745e-01,  1.3820e-01,\n",
      "          6.6489e-01, -3.5420e-01, -1.7432e-01, -7.4743e-02,  9.8688e-02,\n",
      "         -7.9480e-01, -2.4369e-01, -3.6521e-01, -9.9983e-01,  4.3709e-01,\n",
      "         -9.9999e-01,  8.3905e-01, -6.2768e-01, -2.3021e-01,  7.9921e-01,\n",
      "          8.7791e-01,  8.3595e-01, -3.7244e-01, -1.3298e-01,  8.0565e-01,\n",
      "          6.3136e-01, -2.7483e-01, -7.1418e-01, -7.6895e-01,  2.3862e-01,\n",
      "         -2.9225e-01,  2.3352e-01, -8.2974e-01,  6.3558e-01, -9.8118e-02,\n",
      "          9.9999e-01,  3.1860e-02, -7.4577e-01, -6.1133e-01,  2.2803e-01,\n",
      "         -2.9858e-01,  9.9998e-01,  4.4517e-01, -9.1300e-01,  2.3316e-01,\n",
      "         -7.9483e-01, -8.6786e-01,  3.4849e-01,  1.0099e-01, -9.0200e-01,\n",
      "         -9.6737e-01,  7.3687e-01, -2.2603e-01,  2.6478e-02,  8.1330e-01,\n",
      "         -4.4288e-01, -4.7871e-01,  1.4553e-01,  8.7113e-01,  9.8337e-01,\n",
      "          6.8654e-01, -1.7831e-01, -9.5969e-01, -8.4593e-01,  9.3412e-01,\n",
      "          3.1785e-01, -8.0846e-01,  1.2500e-01,  9.9997e-01,  1.6564e-01,\n",
      "         -6.0288e-01, -7.9066e-01, -4.2281e-01, -2.5177e-01, -3.0951e-01,\n",
      "          1.4528e-01,  4.4903e-01,  8.9978e-01, -2.4534e-01,  8.9945e-01,\n",
      "         -7.3913e-01,  1.8935e-01, -6.8380e-01,  1.3701e-02,  3.4984e-01,\n",
      "         -9.3457e-01, -9.4083e-01, -9.6354e-01,  8.1986e-01,  9.4381e-02,\n",
      "         -3.3640e-02,  2.1123e-01, -3.8520e-02, -7.6187e-03,  3.0089e-01,\n",
      "         -1.0000e+00,  6.1895e-01,  7.7653e-02,  8.5359e-02,  8.9273e-01,\n",
      "          6.1807e-01,  6.7957e-01,  1.2642e-01, -9.3967e-01, -6.4693e-01,\n",
      "         -8.6947e-02, -2.6341e-02,  2.3204e-01,  2.1015e-01,  5.9909e-01,\n",
      "         -7.3336e-03,  2.5409e-03, -7.6107e-01, -8.3955e-01, -9.8213e-01,\n",
      "         -9.5986e-01,  4.5888e-01,  3.3789e-01,  1.4326e-01,  9.4877e-01,\n",
      "         -7.3113e-01, -6.5071e-02, -1.7957e-01, -8.7991e-01, -5.8790e-01,\n",
      "         -1.7341e-01, -1.4980e-01, -3.5359e-02,  4.4392e-01,  6.9262e-01,\n",
      "         -1.2488e-01,  8.8683e-01, -1.6740e-01, -2.2563e-01, -7.1986e-01,\n",
      "          2.4781e-01,  9.7137e-01, -7.6006e-01,  2.1710e-01,  7.3684e-01,\n",
      "          5.4538e-02,  2.5561e-03, -1.4622e-01,  4.7930e-02,  9.0399e-01,\n",
      "         -1.6780e-01,  5.4122e-01, -3.9526e-01, -2.3460e-01, -1.1035e-01,\n",
      "         -1.1239e-01, -1.3890e-01, -8.2061e-01, -2.1287e-01, -1.4135e-01,\n",
      "          6.4764e-01,  9.1307e-01, -1.1900e-01, -1.7872e-01, -1.1444e-01,\n",
      "          3.8873e-01, -7.7606e-01, -5.8416e-01, -6.7458e-02,  7.0067e-01,\n",
      "          9.1118e-01,  7.2843e-02,  9.7861e-01, -3.0984e-01, -1.2749e-01,\n",
      "          1.5130e-01,  6.5153e-02, -7.4904e-02, -2.8183e-01, -3.0492e-01,\n",
      "         -6.7133e-01,  4.2227e-01,  4.6784e-01,  9.9993e-01,  9.6362e-02,\n",
      "          1.7944e-01, -5.1807e-01, -5.9217e-01, -6.3458e-03, -7.9777e-01,\n",
      "         -9.9999e-01,  2.0173e-02, -6.6528e-01,  3.9324e-01, -5.3961e-01,\n",
      "          7.4424e-01, -5.7524e-01, -4.3121e-01, -3.7797e-01,  8.5368e-01,\n",
      "          8.2492e-01, -1.8460e-01, -3.6823e-01, -2.7706e-01, -5.2298e-01,\n",
      "          9.7144e-01,  1.2661e-01,  7.2548e-01,  7.3991e-01, -2.6014e-01,\n",
      "         -7.2660e-01, -4.2882e-01,  4.5701e-01]], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(np.expand_dims(np.arange(128), 0), dtype=torch.long)\n",
    "\n",
    "input_shape = input_ids.size()\n",
    "seq_length = input_shape[1]\n",
    "position_ids = torch.arange(seq_length, dtype=torch.long)\n",
    "token_type_ids = torch.zeros(input_shape, dtype=torch.long)\n",
    "\n",
    "out = bertmodel(input_ids)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
